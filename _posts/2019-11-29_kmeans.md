---
layout: post
title: An Application of K-Means Clustering
---

This post highlights a simple application of unsupervised learning using K-means to gain insight into a set of data. Consider a set of data that is being described by two factors, as shown in the figure:

![Plot of data set along two axes. There appears to be three distinct clusters.]({{ site.baseurl }}/images/kmeans_initial_look.png "Plot of data set along two axes.")

The data appears to belong to one of three groups. If we are interested creating predicition models of this relationship, it could be advantageous to subset the data by its respective group before building the prediction models. The K-means algorithm is a simple approach to this problem. It assigns each point in the data set to a group based on its Euclidean distance to the mean location of each estimated group. 

To initialize the algorithm, the user defines a number of groups and a starting point for each mean location. The algorithm then follows an iterative process:

- Calculate the Euclidean distance of each point to the estimated means
- Assign a group to each point according to its shortest Euclidean distance
- Recalculate the mean locations based on the newly-assigned groups

This process is repeated until the mean locations coverge to a given tolerance. When the groups are not previously known, like in this example, it is common to randomly chose the initial mean location. This can be applied to the data shown above using sklearn's clustering library in Python:

```Python code

# dat - the data set has been preloaded into Python kernel
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit( dat.transpose() )

fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,7))
ax1.scatter(d[0,:],d[1,:],c=kmeans.labels_, cmap='cividis')
```

![Clustering results via K means.]({{ site.baseurl }}/images/kmeans_results.png "Clustering results via K means compared against "true" clusters.")

From here, the data can be separated based on its labels - for ``sklearn`` they are returned as a class attribute. This approach is simple, it converges quickly, and does not assume any information about the distrubition of the data. Because of its simplicity, however, the data does have a possibility of being mislabeled. Since the algorithm clusters according to Euclidean distance, there is a potential for mislabeling if there is not clear linear separability in the data. This can be seen from the example. In a case where the distribution of the data can be assumed (the example appears to be multivariate Gaussian), clustering by Gaussian Mixture Models may be more appropriate.
